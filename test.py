import torch
import time
import sys
import urllib.parse
import platform
from traceback import print_exc


def get_accelerator_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name(0)
        total_memory_gb = (
            torch.cuda.get_device_properties(0).total_memory / 1024**3
        )
        return device, device_name, total_memory_gb

    mps_backend = getattr(torch.backends, "mps", None)
    if mps_backend is not None and mps_backend.is_available():
        device = torch.device("mps")
        device_name = "Apple MPS"
        return device, device_name, None

    return None, None, None


def synchronize_device(device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()


def empty_device_cache(device):
    if device.type == "cuda":
        torch.cuda.empty_cache()
    elif device.type == "mps":
        torch.mps.empty_cache()


def generate_github_issue_link(device_name, results):
    """ç”ŸæˆGitHub issueé“¾æ¥ï¼ŒåŒ…å«é¢„å¡«å……çš„æµ‹è¯•æ•°æ®"""

    # è·å–ç³»ç»Ÿä¿¡æ¯
    python_version = (
        f"py{sys.version_info.major}{sys.version_info.minor}{sys.version_info.micro}"
    )
    torch_version = f"torch{torch.__version__.replace('.', '').replace('+', '')}"

    # æ„å»ºissueæ ‡é¢˜
    title = f"æ–°å¢æ€§èƒ½æ•°æ®ï¼š{device_name}"

    # æ·»åŠ æ€§èƒ½æ•°æ®
    fp32_result = results.get("FP32", "N/A")
    fp16_result = results.get("FP16", "N/A")
    bf16_result = results.get("BF16", "N/A")
    fp8_result = results.get("FP8 E4M3FN", "N/A")

    if fp32_result != "N/A":
        fp32_result = f"{fp32_result:.2f}"
    if fp16_result != "N/A":
        fp16_result = f"{fp16_result:.2f}"
    if bf16_result != "N/A":
        bf16_result = f"{bf16_result:.2f}"
    if fp8_result != "N/A":
        fp8_result = f"{fp8_result:.2f}"

    # æ„å»ºç®€åŒ–çš„issueå†…å®¹
    body = f"""## è®¾å¤‡ä¿¡æ¯
- è®¾å¤‡åç§°ï¼š{device_name}
- Pythonç‰ˆæœ¬ï¼š{python_version}
- PyTorchç‰ˆæœ¬ï¼š{torch_version}

## æ€§èƒ½æ•°æ®
```
| {device_name} | {fp32_result} | {fp16_result} | {bf16_result} | {fp8_result} | **è¯·å¡«å†™note** | **è¯·å¡«å†™contributor** |
```

## å¡«å†™è¯´æ˜
1. **noteåˆ—**ï¼šè¯·å¡«å†™æµ‹è¯•ç¯å¢ƒï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®å­—ä¼šè‡ªåŠ¨å½’ç±»ï¼š
   - `GCP` (GCPäº‘å®ä¾‹)
   - `å®ä½“æœº` (ç‰©ç†æœºå™¨)
   - `ç¬”è®°æœ¬` (ç¬”è®°æœ¬ç”µè„‘)  
   - `docker` (Dockerå®¹å™¨)
   - `ä¼˜äº‘æ™ºç®—` (ä¼˜äº‘æ™ºç®—å¹³å°)
   - `æ™ºç®—äº‘æ‰‰` (æ™ºç®—äº‘æ‰‰å¹³å°)

2. **contributoråˆ—**ï¼šæ ¼å¼ä¸º `[ç”¨æˆ·å](https://github.com/ç”¨æˆ·å)`ï¼Œä¸å¡«é»˜è®¤ä½ è‡ªå·±

æ„Ÿè°¢æ‚¨çš„è´¡çŒ®ï¼"""

    # URLç¼–ç 
    encoded_title = urllib.parse.quote(title)
    encoded_body = urllib.parse.quote(body)

    # ç”ŸæˆGitHub issueé“¾æ¥
    issue_url = f"https://github.com/zzc0721/torch-performance-test-data/issues/new?title={encoded_title}&body={encoded_body}"

    print(f"\n{'=' * 60}")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æ€§èƒ½æ•°æ®æ‘˜è¦ï¼š")
    print(f"è®¾å¤‡ï¼š{device_name}")
    print(
        f"FP32: {fp32_result} TFLOPS | FP16: {fp16_result} TFLOPS | BF16: {bf16_result} TFLOPS | FP8: {fp8_result} TFLOPS"
    )
    print("\nğŸ”— æäº¤æ•°æ®è¯·ç‚¹å‡»ä»¥ä¸‹é“¾æ¥ï¼š")
    print(f"{issue_url}")
    print(f"\n{'=' * 60}")
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. ç‚¹å‡»é“¾æ¥ä¼šè‡ªåŠ¨å¡«å……è®¾å¤‡ä¿¡æ¯å’Œæ€§èƒ½æ•°æ®")
    print("2. è¯·åœ¨issueä¸­å¡«å†™noteï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰å’Œcontributorä¿¡æ¯")
    print("3. åŒ…å«ç‰¹å®šå…³é”®å­—çš„noteå°†è¢«è‡ªåŠ¨å½’ç±»")


def benchmark_precision(precision, matrix_size, device, warmup=6, test_iters=30):
    if device.type not in ("cuda", "mps"):
        raise RuntimeError("å½“å‰ä»…æ”¯æŒCUDAæˆ–MPSè®¾å¤‡è¿›è¡Œæµ‹è¯•")

    # åˆå§‹åŒ–çŸ©é˜µ
    try:
        if precision == torch.int8:
            # INT8 ç‰¹æ®Šå¤„ç†
            a = torch.randint(
                -128, 127, (matrix_size, matrix_size), dtype=precision, device=device
            )
            b = torch.randint(
                -128, 127, (matrix_size, matrix_size), dtype=precision, device=device
            )
        elif precision == torch.float8_e4m3fn:
            # FP8 ç‰¹æ®Šå¤„ç†
            a = torch.randn(matrix_size, matrix_size, device=device)
            b = torch.randn(matrix_size, matrix_size, device=device)
            # é˜²æ­¢å…¨ä¸º0ï¼Œé‡æ–°èµ‹å€¼éé›¶éšæœºæ•°
            a = a + torch.randn_like(a) * 1e-3
            b = b + torch.randn_like(b) * 1e-3
            a = a.to(dtype=precision)
            b = b.to(dtype=precision)
        else:
            a = torch.randn(matrix_size, matrix_size, dtype=precision, device=device)
            b = torch.randn(matrix_size, matrix_size, dtype=precision, device=device)
    except RuntimeError as e:
        message = str(e).lower()
        if "not implemented" in message or "not support" in message:
            return None
        print_exc()
        raise

    # é¢„çƒ­
    for _ in range(warmup):
        torch.mm(a, b)
    synchronize_device(device)

    # æ­£å¼æµ‹è¯•
    start_time = time.time()
    for _ in range(test_iters):
        torch.mm(a, b)
    synchronize_device(device)
    elapsed = time.time() - start_time

    # è®¡ç®—FLOPS
    flops_per_iter = 2 * matrix_size**3
    total_flops = flops_per_iter * test_iters
    tflops = (total_flops / elapsed) / 1e12

    # æ¸…ç†æ˜¾å­˜
    del a, b
    empty_device_cache(device)

    return tflops


if __name__ == "__main__":
    device, device_name, total_memory_gb = get_accelerator_device()
    if device is None:
        print("æœªæ£€æµ‹åˆ°CUDAæˆ–MPSè®¾å¤‡")
        exit(1)

    print(f"æµ‹è¯•è®¾å¤‡: {device_name}")
    if total_memory_gb is not None:
        print(f"æ˜¾å­˜å¤§å°: {total_memory_gb:.1f} GB\n")
    elif device.type == "mps":
        print("ä½¿ç”¨Apple MPSå›¾å½¢åŠ é€Ÿå™¨\n")

    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    # æµ‹è¯•ä¸åŒç²¾åº¦å’ŒçŸ©é˜µå¤§å°
    matrix_sizes = [1024, 2048, 4096, 8192, 10240]
    precisions = [
        ("FP32", torch.float32),
        ("FP16", torch.float16),
        ("BF16", torch.bfloat16),
        # ("INT8", torch.int8),  # å¯é€‰ï¼šå¦‚æœéœ€è¦æµ‹è¯•INT8
    ]

    try:
        fp8_precision = torch.float8_e4m3fn
    except AttributeError:
        fp8_precision = None
        print("PyTorch å½“å‰ä¸æ”¯æŒ FP8 E4M3FNï¼Œè·³è¿‡è¯¥é¡¹æµ‹è¯•")

    if fp8_precision is not None:
        precisions.append(("FP8 E4M3FN", fp8_precision))

    results = {}
    for precision_name, precision in precisions:
        print(f"\næµ‹è¯• {precision_name}:")
        results[precision_name] = []

        for size in matrix_sizes:
            print(f"æµ‹è¯•çŸ©é˜µå¤§å°: {size}x{size}")
            tflops = benchmark_precision(precision, size, device)
            if tflops is not None:
                results[precision_name].append((size, tflops))
                print(f"  æ€§èƒ½: {tflops:.2f} TFLOPS")
            else:
                print("  ä¸æ”¯æŒæ­¤ç²¾åº¦")
                break

    # æ‰“å°æ€»ç»“
    print("\næ€§èƒ½æ€»ç»“:")
    best_results = {}
    for precision_name, measurements in results.items():
        if measurements:
            max_tflops = max(tflops for _, tflops in measurements)
            best_results[precision_name] = max_tflops
            print(f"{precision_name} æœ€å¤§ç®—åŠ›: {max_tflops:.2f} TFLOPS")

    # ç”ŸæˆGitHub issueé“¾æ¥
    generate_github_issue_link(device_name, best_results)

import torch
import time
import sys
import urllib.parse
import platform
from traceback import print_exc


def get_accelerator_device():
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name(0)
        total_memory_gb = (
            torch.cuda.get_device_properties(0).total_memory / 1024**3
        )
        return device, device_name, total_memory_gb

    mps_backend = getattr(torch.backends, "mps", None)
    if mps_backend is not None and mps_backend.is_available():
        device = torch.device("mps")
        device_name = "Apple MPS"
        return device, device_name, None

    return None, None, None


def synchronize_device(device):
    if device.type == "cuda":
        torch.cuda.synchronize()
    elif device.type == "mps":
        torch.mps.synchronize()


def empty_device_cache(device):
    if device.type == "cuda":
        torch.cuda.empty_cache()
    elif device.type == "mps":
        torch.mps.empty_cache()


def generate_github_issue_link(device_name, results, detailed_results):
    """ç”ŸæˆGitHub issueé“¾æ¥ï¼ŒåŒ…å«é¢„å¡«å……çš„æµ‹è¯•æ•°æ®"""

    # è·å–ç³»ç»Ÿä¿¡æ¯
    python_version = (
        f"py{sys.version_info.major}{sys.version_info.minor}{sys.version_info.micro}"
    )
    torch_version = f"torch{torch.__version__.replace('.', '').replace('+', '')}"

    # æ„å»ºissueæ ‡é¢˜
    title = f"æ–°å¢æ€§èƒ½æ•°æ®ï¼š{device_name}"

    # æ·»åŠ æ€§èƒ½æ•°æ®
    fp32_result = results.get("FP32", "N/A")
    tf32_result = results.get("TF32", "N/A")
    fp16_result = results.get("FP16", "N/A")
    bf16_result = results.get("BF16", "N/A")

    if fp32_result != "N/A":
        fp32_result = f"{fp32_result:.2f}"
    if tf32_result != "N/A":
        tf32_result = f"{tf32_result:.2f}"
    if fp16_result != "N/A":
        fp16_result = f"{fp16_result:.2f}"
    if bf16_result != "N/A":
        bf16_result = f"{bf16_result:.2f}"

    # æ„å»ºè¯¦ç»†æ€§èƒ½æ•°æ®è¡¨æ ¼
    perf_details = "\n## è¯¦ç»†æ€§èƒ½æ•°æ®\n```\n"
    for precision_name, measurements in detailed_results.items():
        if measurements:
            perf_details += f"\n{precision_name}:\n"
            for size, tflops in measurements:
                perf_details += f"  {size}x{size}: {tflops:.2f} TFLOPS\n"
    perf_details += "```\n"

    # æ„å»ºç®€åŒ–çš„issueå†…å®¹
    body = f"""## è®¾å¤‡ä¿¡æ¯
- è®¾å¤‡åç§°ï¼š{device_name}
- Pythonç‰ˆæœ¬ï¼š{python_version}
- PyTorchç‰ˆæœ¬ï¼š{torch_version}

## æ€§èƒ½æ•°æ®
```
| {device_name} | {fp32_result} | {tf32_result} | {fp16_result} | {bf16_result} | **è¯·å¡«å†™note** | **è¯·å¡«å†™contributor** |
```

{perf_details}

## å¡«å†™è¯´æ˜
1. **noteåˆ—**ï¼šè¯·å¡«å†™æµ‹è¯•ç¯å¢ƒï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®å­—ä¼šè‡ªåŠ¨å½’ç±»ï¼š
   - `GCP` (GCPäº‘å®ä¾‹)
   - `å®ä½“æœº` (ç‰©ç†æœºå™¨)
   - `ç¬”è®°æœ¬` (ç¬”è®°æœ¬ç”µè„‘)
   - `docker` (Dockerå®¹å™¨)
   - `ä¼˜äº‘æ™ºç®—` (ä¼˜äº‘æ™ºç®—å¹³å°)
   - `æ™ºç®—äº‘æ‰‰` (æ™ºç®—äº‘æ‰‰å¹³å°)

2. **contributoråˆ—**ï¼šæ ¼å¼ä¸º `[ç”¨æˆ·å](https://github.com/ç”¨æˆ·å)`ï¼Œä¸å¡«é»˜è®¤ä½ è‡ªå·±

æ„Ÿè°¢æ‚¨çš„è´¡çŒ®ï¼"""

    # URLç¼–ç 
    encoded_title = urllib.parse.quote(title)
    encoded_body = urllib.parse.quote(body)

    # ç”ŸæˆGitHub issueé“¾æ¥
    issue_url = f"https://github.com/zzc0721/torch-performance-test-data/issues/new?title={encoded_title}&body={encoded_body}"

    print(f"\n{'=' * 60}")
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æ€§èƒ½æ•°æ®æ‘˜è¦ï¼š")
    print(f"è®¾å¤‡ï¼š{device_name}")
    print(
        f"FP32: {fp32_result} TFLOPS | TF32: {tf32_result} TFLOPS | FP16: {fp16_result} TFLOPS | BF16: {bf16_result} TFLOPS"
    )

    # å¦‚æœæœ‰ FP8 æ•°æ®ï¼Œå•ç‹¬æ˜¾ç¤º
    fp8_result = results.get("FP8 E4M3FN", "N/A")
    if fp8_result != "N/A":
        print(f"FP8: {fp8_result:.2f} TFLOPS")
    print("\nğŸ”— æäº¤æ•°æ®è¯·ç‚¹å‡»ä»¥ä¸‹é“¾æ¥ï¼š")
    print(f"{issue_url}")
    print(f"\n{'=' * 60}")
    print("ğŸ’¡ æç¤ºï¼š")
    print("1. ç‚¹å‡»é“¾æ¥ä¼šè‡ªåŠ¨å¡«å……è®¾å¤‡ä¿¡æ¯å’Œæ€§èƒ½æ•°æ®")
    print("2. è¯·åœ¨issueä¸­å¡«å†™noteï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰å’Œcontributorä¿¡æ¯")
    print("3. åŒ…å«ç‰¹å®šå…³é”®å­—çš„noteå°†è¢«è‡ªåŠ¨å½’ç±»")


def benchmark_precision(precision, matrix_size, device, warmup=10, test_iters=30):
    if device.type not in ("cuda", "mps"):
        raise RuntimeError("å½“å‰ä»…æ”¯æŒCUDAæˆ–MPSè®¾å¤‡è¿›è¡Œæµ‹è¯•")

    # åˆå§‹åŒ–çŸ©é˜µ
    try:
        if precision == torch.int8:
            # INT8 ç‰¹æ®Šå¤„ç†
            a = torch.randint(
                -128, 127, (matrix_size, matrix_size), dtype=precision, device=device
            )
            b = torch.randint(
                -128, 127, (matrix_size, matrix_size), dtype=precision, device=device
            )
        elif precision == torch.float8_e4m3fn:
            # FP8 ç‰¹æ®Šå¤„ç†
            a = torch.randn(matrix_size, matrix_size, device=device)
            b = torch.randn(matrix_size, matrix_size, device=device)
            # é˜²æ­¢å…¨ä¸º0ï¼Œé‡æ–°èµ‹å€¼éé›¶éšæœºæ•°
            a = a + torch.randn_like(a) * 1e-3
            b = b + torch.randn_like(b) * 1e-3
            a = a.to(dtype=precision)
            b = b.to(dtype=precision)
        else:
            a = torch.randn(matrix_size, matrix_size, dtype=precision, device=device)
            b = torch.randn(matrix_size, matrix_size, dtype=precision, device=device)
    except RuntimeError as e:
        message = str(e).lower()
        if "not implemented" in message or "not support" in message:
            return None
        print_exc()
        raise

    # é¢„çƒ­
    for _ in range(warmup):
        torch.mm(a, b)
    synchronize_device(device)

    # æ­£å¼æµ‹è¯• - ä½¿ç”¨ CUDA Event è¿›è¡Œç²¾ç¡®è®¡æ—¶
    if device.type == "cuda":
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)

        start_event.record()
        for _ in range(test_iters):
            torch.mm(a, b)
        end_event.record()
        torch.cuda.synchronize()

        elapsed = start_event.elapsed_time(end_event) / 1000.0  # è½¬æ¢ä¸ºç§’
    else:
        # MPS ä½¿ç”¨åŸæœ‰çš„ time.time() æ–¹å¼
        start_time = time.time()
        for _ in range(test_iters):
            torch.mm(a, b)
        synchronize_device(device)
        elapsed = time.time() - start_time

    # è®¡ç®—FLOPS
    flops_per_iter = 2 * matrix_size**3
    total_flops = flops_per_iter * test_iters
    tflops = (total_flops / elapsed) / 1e12

    # æ¸…ç†æ˜¾å­˜
    del a, b
    empty_device_cache(device)

    return tflops


if __name__ == "__main__":
    device, device_name, total_memory_gb = get_accelerator_device()
    if device is None:
        print("æœªæ£€æµ‹åˆ°CUDAæˆ–MPSè®¾å¤‡")
        exit(1)

    print(f"æµ‹è¯•è®¾å¤‡: {device_name}")
    if total_memory_gb is not None:
        print(f"æ˜¾å­˜å¤§å°: {total_memory_gb:.1f} GB\n")
    elif device.type == "mps":
        print("ä½¿ç”¨Apple MPSå›¾å½¢åŠ é€Ÿå™¨\n")

    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    # æ˜¾å¼è®¾ç½® TF32 æ ‡å¿—ï¼ˆé¿å… Ampere+ æ˜¾å¡çš„ä¼ª FP32 é—®é¢˜ï¼‰
    if device.type == "cuda" and hasattr(torch.backends.cuda, "matmul"):
        print(f"\nå½“å‰ TF32 è®¾ç½®:")
        print(f"  matmul.allow_tf32 = {torch.backends.cuda.matmul.allow_tf32}")
        print(f"  cudnn.allow_tf32 = {torch.backends.cudnn.allow_tf32}")

    # æµ‹è¯•ä¸åŒç²¾åº¦å’ŒçŸ©é˜µå¤§å°
    matrix_sizes = [1024, 2048, 4096, 8192, 10240]
    precisions = [
        ("FP32", torch.float32, False),  # ç¦ç”¨ TF32
        ("TF32", torch.float32, True),   # å¯ç”¨ TF32ï¼ˆä»… Ampere+ æ˜¾å¡ï¼‰
        ("FP16", torch.float16, None),   # FP16 ä¸å— TF32 å½±å“
        ("BF16", torch.bfloat16, None),  # BF16 ä¸å— TF32 å½±å“
        # ("INT8", torch.int8, None),    # å¯é€‰ï¼šå¦‚æœéœ€è¦æµ‹è¯•INT8
    ]

    try:
        # æ£€æŸ¥FP8ç±»å‹æ˜¯å¦å­˜åœ¨å¹¶ä¸”å®é™…å¯ç”¨
        test_a = torch.randn(2, 2, device=device).to(torch.float8_e4m3fn)
        test_b = torch.randn(2, 2, device=device).to(torch.float8_e4m3fn)
        _ = torch.mm(test_a, test_b)  # æµ‹è¯•å®é™…è¿ç®—
        fp8_precision = torch.float8_e4m3fn
        del test_a, test_b
        precisions.append(("FP8 E4M3FN", fp8_precision, None))
    except (AttributeError, RuntimeError) as e:
        print("\nPyTorch å½“å‰ä¸æ”¯æŒ FP8 E4M3FNï¼Œè·³è¿‡è¯¥é¡¹æµ‹è¯•")
        if isinstance(e, RuntimeError):
            print(f"  åŸå› : {str(e)}")

    results = {}
    for precision_name, precision, tf32_setting in precisions:
        # è®¾ç½® TF32ï¼ˆä»…å¯¹ FP32 ç²¾åº¦ï¼‰
        if device.type == "cuda" and tf32_setting is not None and hasattr(torch.backends.cuda, "matmul"):
            torch.backends.cuda.matmul.allow_tf32 = tf32_setting
            torch.backends.cudnn.allow_tf32 = tf32_setting
            print(f"\n[{precision_name}] å·²è®¾ç½® TF32 = {tf32_setting}")

        print(f"\næµ‹è¯• {precision_name}:")
        results[precision_name] = []

        for size in matrix_sizes:
            # é¿å…åœ¨å¾ªç¯å†…æ‰“å°è¿‡å¤šï¼Œå‡å°‘éšå¼åŒæ­¥
            tflops = benchmark_precision(precision, size, device)
            if tflops is not None:
                results[precision_name].append((size, tflops))
            else:
                print(f"  ä¸æ”¯æŒæ­¤ç²¾åº¦")
                break

        # åœ¨è¯¥ç²¾åº¦æ‰€æœ‰æµ‹è¯•å®Œæˆåç»Ÿä¸€æ‰“å°
        if results[precision_name]:
            print(f"  å®Œæˆ {len(results[precision_name])} ä¸ªå°ºå¯¸æµ‹è¯•")

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æ€§èƒ½æ€»ç»“:")
    best_results = {}
    for precision_name, measurements in results.items():
        if measurements:
            best_size, max_tflops = max(measurements, key=lambda x: x[1])
            best_results[precision_name] = max_tflops
            print(f"{precision_name:12} æœ€å¤§ç®—åŠ›: {max_tflops:7.2f} TFLOPS @ {best_size}x{best_size}")
    print("=" * 60)

    # ç”ŸæˆGitHub issueé“¾æ¥
    generate_github_issue_link(device_name, best_results, results)
